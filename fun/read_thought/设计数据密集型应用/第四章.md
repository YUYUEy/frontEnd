### Encoding and Evolution 数据编码和程序演化     

很少有程序的一次性就写好后期不变化的。新功能的添加都意味着就程序要改变。 并且大多数情况下数据也要随着程序改变。可能需要    
添加新的字段，可能需要用新的方式来保存。在系统设计之初 应该让系统变得够灵活。以适应后面的一些变化。     

如果使用关系数据库的话是通过alter来改变。 如果是文档型数据库的话 要考虑到文档数据库 读时模式的特点。数据可能混合了   
新的数据格式 和 旧的格式。这个时候就要考虑兼容性了。     

**兼容性：**    

* 向后兼容     

新版本的代码理解旧版本的数据。

* 向前兼容    

旧版本的代码来理解新版本的数据,这种兼容方式会麻烦点。从这里也引出了各种数据编码格式。   

### 数据编码格式      

下面说到的有 json xml Protocol Buffers，Thrift和Avro。 格式说完后还会说 如何通过这些来进行通信和存储。     

广义来讲程序主要用两种格式，一种是内存中直接使用的，另一种是通信 存储用的。      

内存中数据存在 列表 数组 字典 结构体 对象中     

保存在文件中对其持久化就需要进行encode 例如 json xml 等..

### 语言特定的格式     

很多编程语言都有序列化支持，python 中有pickle， java中有java.io.Serializable。 这些方法在各自的语言中    
可以很好的使用，但是很多时候这些工具是和语言绑死在一起的，如果只是很简单的用一下 用这些内置的编码也无妨，    
但是如何数据会被频繁使用，并且可能会和其他系统交互 就要考虑使用其它更为通用的编码方式了。     


### json xml csv 二进制编码    

先来看 json（JS 对象标记） 和 xml（可扩展标记语言） csv     

**json xml csv 对比**
    
    xml    element、attribute和element content。
    json   object、array、string、number、boolean(true/false)和null。
    csv  逗号分割的数据
    
    为什么有时候json更好用？
    
    表现一个学生对象
    
    每个 key value 用属性来表示是可以的，但是如果value也是一个object 就无法作为属性（attribute）来表示。
    <student name="John" age="10">
        <address>
            <country>China</country>
            <province>Guang Dong</province>
            <city>...</city>
            <district>...</district>
            ...
        </address>
    </student>

    反观json 可以很自然的映射
    {
        "name": "John",
        "age" : 10,
        "address" : {
            "country" : "China",
            "province" : "Guang Dong",
            "city" : "..",
            "district" : "..",
            ...
        }
    }

    另外 xml映射数组tag会冗余很多。
    
    xml 比json 好的地方。
    有规范的xpath支持，在很复杂的数据中找到自己需要的数据会比较容易。

**其它问题**  

1. 数字的编码多有歧义之处。XML和CSV不能区分数字和字符串（除非引用外部模式）。      
JSON虽然区分字符串和数字，但不区分整数和浮点数，而且不能指定精度。    

2. json xml 对unicode支持很好的支持，但是对二进制串支持不好。 这一点可以通过base64     
来转换，但是增加了数据的大小。    

3. csv 的格式比较模糊，如果一个值包含了逗号，或者换行符。 这个要如何处理（csv是通过逗号来将值分割的。）

json，csv，xml 虽然有一些缺陷，但是最大的问题不是在于这些，最大的问题是让不同的人认可一套工具来使用。     
开始实习后接触了很多的接口，遇到的json 和 xml各占一半。使用的静态数据的话基本都是csv 和 json格式的。      
如果对方给的csv格式的数据并不是那么规范的话，处理起来很蛋疼。。


 
### 二进制编码      

当数据到达一个规模后，不同的数据格式的选择就会带来很大的影响。常用的mongodb 使用的数据格式就是类似json的bson。    

**Thrift与Protocol Buffers ,Avro** 这三种编码的共同点都是有一个预先的schema来定义数据的格式，在后面保存数据的时候     
一些重复的字段就不需要再次保存了。 通过这种方式来减少空间。    

主要来看下avro这种方式，看一个它如何支持模式演变的。    

使用avro 来写的模式语言如下：    

    record Person {
        string                userName;
        union { null, long }  favoriteNumber = null;
        array<string>         interests;
    }

    等价的json

    {
        "type": "record",
        "name": "Person",
        "fields": [
            {"name": "userName", "type": "string"},
            {"name": "favoriteNumber", "type": ["null", "long"], "default": null},
            {"name": "interests", "type": {"type": "array", "items": "string"}
        ] 
    }

**avro 如何支持模式演变的？**      

读者模式 和 写者模式， 读者模式指的是要解码数据的时候（从网络 数据库 文件读取）， 这些数据是在某一个    
模式中的。 而写者模式是 编码的时候，用它知道的模式来编码。      

模式演变的关键也就在于 读者模式 和 写者模式不必相同，只需要兼容就可以了。
如果读取数据的时候，遇到在写者模式中出现，而不在读者模式中出现的就忽略，如果在读者模式出现，不在写者模式的     
就填充默认值。 

![](http://pyblog-10073407.image.myqcloud.com/postimage1521375334?imageView2/0/w/450/h/400)


#### 模式演变的规则     

上面也提到了 向前兼容就是前面的代码来兼容后面的数据。 向后兼容正好相反。 使用avro的时候是这样子做的      
向前兼容： 使用新版本的架构作为写者，而旧版本的作为读者。 向后兼容则是用新版本的作为读者， 写者作为旧版本。      

并且在保证兼容性的时候只可以添加 删除有默认值的字段，试想如果添加了一个没有默认值的字段，新的阅读器无法读取     
旧作者写的数据。如果把默认的数据删除掉，旧的阅读器也无法读取新的写者的数据。

这些schema 都可以集中保存在一个地方，当程序出现变化的时候处理起来也很方便。并且在avro中 读的schema 和 写的schema    
是可以不同的，avro会自己进行映射。 对于静态语言来说avro可以自动生成这些语言的代码，并且在编译的时候还原数据。       

    二进制编码的好处：     
    比二进制json更省空间（主要是省略了字段名）
    对于静态语言来说，可以直接从模型生成代码
    可以保证向前 向后兼容
    


#### 数据流

有一些数据可能是在A进程处理完后发送到B进程进行处理。A B进程可能不是共享一个内存空间的。这时     
就需要把数据通过网络（A进程 encode---网络---decode B进程）来处理。 兼容性就是 A B 两端的一个关系。     

常见的有三种方式：   
* 通过数据库   
* 通过服务调用 rpc rest
* 异步消息传输

**数据库中的数据流**    

在一个应用中，可能是A程序来写数据，B程序来读取数据。也可能是新版本的代码写旧版本的数据。     
很常见的一种情况是不同的进程同时访问数据库。或者是在一个大的应用中，执行滚动升级，有的进程    
跑新的代码，有的跑旧的代码。 一个数据库的值可能会被新版本的代码写入，被旧版本的代码读取。     
数据库在这里就需要做向前兼容。     

在一些工具中会对这些东西进行检查， 比如django，对model进行改变的时候程序会提醒你 这个字段是否有默认值    
是否可以为空。这都是为了保证兼容性。       

如果不注意这些事情就可能出错，比如旧版本的程序处理新版本程序写入的数据。处理完后可能数据会丢失。    
    
![](http://pyblog-10073407.image.myqcloud.com/postimage1521474313?imageView2/0/w/450/h/400)         


之前做一个程序的时候就踩过这种坑，一个隔了一段时间没写的程序需要添加一个字段，并且这个字段是没有默认值的。      
这就导致旧的那些数据的这个字段是拿不到数据的。出现了奇奇怪怪的问题。      


**服务中的数据流：REST 与 RPC**      

使用网络服务的时候，最常见的两种角色是 客户端 服务器，服务器公开的api就是服务。用户通过客户端进行请求，服务器    
根据请求进行解析，响应。     

常用的两种web服务，rest 和 soap，rest的服务数据格式简单，并且可以使用很多http提供的功能，比如缓存，身份验证，内容类型    
通过url 来控制版本号。大家习惯叫rest 实现的api为restful。（平常使用python的时候接触的大多数都是restful 接口）     


**远程过程调用（RPC）的问题**     

一个高性能的，用python实现的rpc库（https://github.com/ray-project/ray）      

RPC的实现都是尽可能的包装成本地函数，看起来是在本地使用。但是问题主要有：      

* RPC通过网络来调度，网络是不可靠的
* 有的时候出现问题难以判断是网络的问题还是函数的问题
* 对于失败的请求，可能需要多次重试，这里要考虑幂等性
* 需要编码 解码     

**消息传递中的数据流**     

上面说到的一种是 A进程请求B进程，期望B进程尽快响应，一种是一个进程写入数据，另一个进程来读取数据。      
这两者结合起来就是一个 异步消息传输系统，也叫消息队列。 相当于是A B 进程间的一个大缓冲区。    

和RPC相比:   

* 处理的一方如果挂掉，当缓冲来使用，提高可用性
* 可以将消息重发到已经挂了的进程，防止消息丢失 
* 还可以充当一个代理的左右，保护处理的一方的ip 和 端口号
* 一个消息可以给多个人
* A 不需要考虑 谁来处理，可以是B C D     

由于这是单向的，并且是异步的，想要知道处理结果需要自己进行一些处理。RPC更侧重两端，而消息队列侧重中间。    

常用的中间件有 RabbitMQ Kafka。


总结：    

程序要经常升级，很多程序都是滚动升级的，这里就要考虑兼容性的问题。

* 编程语言的编码仅局限于自己，并且向前 向后兼容支持差
* json xml csv 兼容性要自己来做，并且数据格式不紧凑
* Thrift，Protocol Buffers和Avro 二进制模式的在数据读之前要解码，并且可以生成静态语言的代码

数据流

* 数据库 写入的时候编码，读的时候解码
* rpc rest api 客户端对请求编码，服务器进行解码
* 消息队列 发送方编码 接收方解码

























     

